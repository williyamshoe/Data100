
% Default to the notebook output style

    


% Inherit from the specified cell style.




    
\documentclass[11pt]{article}

    
    
    \usepackage[T1]{fontenc}
    % Nicer default font (+ math font) than Computer Modern for most use cases
    \usepackage{mathpazo}

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % We will generate all images so they have a width \maxwidth. This means
    % that they will get their normal width if they fit onto the page, but
    % are scaled down if they would overflow the margins.
    \makeatletter
    \def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth
    \else\Gin@nat@width\fi}
    \makeatother
    \let\Oldincludegraphics\includegraphics
    % Set max figure width to be 80% of text width, for now hardcoded.
    \renewcommand{\includegraphics}[1]{\Oldincludegraphics[width=.8\maxwidth]{#1}}
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionLabelFormat{nolabel}{}
    \captionsetup{labelformat=nolabel}

    \usepackage{adjustbox} % Used to constrain images to a maximum size 
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage[utf8x]{inputenc} % Allow utf-8 characters in the tex document
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range 
    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    

    
    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatability definitions
    \def\gt{>}
    \def\lt{<}
    % Document parameters
    \title{hw3}
    
    
    

    % Pygments definitions
    
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@fm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sa\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@dl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % Exact colors from NB
    \definecolor{incolor}{rgb}{0.0, 0.0, 0.5}
    \definecolor{outcolor}{rgb}{0.545, 0.0, 0.0}



    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

    \begin{document}
    
    
    \maketitle
    
    

    
    Before you turn this problem in, make sure everything runs as expected.
First, \textbf{restart the kernel} (in the menubar, select
Kernel\(\rightarrow\)Restart) and then \textbf{run all cells} (in the
menubar, select Cell\(\rightarrow\)Run All).

Make sure you fill in any place that says \texttt{YOUR\ CODE\ HERE} or
"YOUR ANSWER HERE", as well as your name and collaborators below:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}1}]:} \PY{n}{NAME} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{William Sheu}\PY{l+s+s2}{\PYZdq{}}
        \PY{n}{COLLABORATORS} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZdq{}}
\end{Verbatim}


    \begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

    \section{Homework 3: Loss
Minimization}\label{homework-3-loss-minimization}

\subsection{Modeling, Estimation and Gradient
Descent}\label{modeling-estimation-and-gradient-descent}

\subsection{Due Date: Tuesday 10/9, 11:59
PM}\label{due-date-tuesday-109-1159-pm}

\subsection{Course Policies}\label{course-policies}

Here are some important course policies. These are also located at
http://www.ds100.org/fa18/.

\textbf{Collaboration Policy}

Data science is a collaborative activity. While you may talk with others
about the homework, we ask that you \textbf{write your solutions
individually}. If you do discuss the assignments with others please
\textbf{include their names} at the top of your solution.

\subsection{This Assignment}\label{this-assignment}

In this homework, we explore modeling data, estimating optimal
parameters and a numerical estimation method, gradient descent. These
concepts are some of the fundamentals of data science and machine
learning and will serve as the building blocks for future projects,
classes, and work.

After this homework, you should feel comfortable with the following:

\begin{itemize}
\item
  Practice reasoning about a model
\item
  Build some intuition for loss functions and how they behave
\item
  Work through deriving the gradient of a loss with respect to model
  parameters
\item
  Work through a basic version of gradient descent.
\end{itemize}

This homework is comprised of completing code, deriving analytic
solutions, writing LaTex and visualizing loss.

\subsection{Submission - IMPORTANT, PLEASE
READ}\label{submission---important-please-read}

For this assignment and future assignments (homework and projects) you
will also submit your free response and plotting questions to
gradescope. To do this, you can download as PDF
(\texttt{File-\textgreater{}Download\ As-\textgreater{}PDF\ via\ Latex\ (.pdf)}).
You are responsible for submitting and tagging your answers in
gradescope. For each free response and plotting question, please
include:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Relevant code used to generate the plot or inform your insights
\item
  The written free response or plot
\end{enumerate}

We are doing this to make it easier on our graders and for you, in the
case you need to submit a regrade request. Gradescope (as of now) is
still better for manual grading.

\subsection{Score breakdown}\label{score-breakdown}

\begin{longtable}[]{@{}ll@{}}
\toprule
Question & Points\tabularnewline
\midrule
\endhead
Question 1a & 1\tabularnewline
Question 1b & 1\tabularnewline
Question 1c & 1\tabularnewline
Question 1d & 1\tabularnewline
Question 1e & 1\tabularnewline
Question 2a & 2\tabularnewline
Question 2b & 1\tabularnewline
Question 2c & 1\tabularnewline
Question 2d & 1\tabularnewline
Question 2e & 1\tabularnewline
Question 2f & 1\tabularnewline
Question 3a & 1\tabularnewline
Question 3b & 3\tabularnewline
Question 3c & 2\tabularnewline
Question 4a & 3\tabularnewline
Question 4b & 1\tabularnewline
Question 4c & 1\tabularnewline
Question 4d & 1\tabularnewline
Question 4e & 1\tabularnewline
Question 5a & 2\tabularnewline
Question 5b & 4\tabularnewline
Question 5c & 0\tabularnewline
Question 5d & 0\tabularnewline
Question 6a & 3\tabularnewline
Question 6b & 3\tabularnewline
Question 6c & 3\tabularnewline
Question 6d & 3\tabularnewline
Question 6e & 3\tabularnewline
Question 6f & 3\tabularnewline
Question 6g & 3\tabularnewline
Question 7a & 1\tabularnewline
Question 7b & 1\tabularnewline
Question 7c & 1\tabularnewline
Question 7d & 1\tabularnewline
Question 7e & 0\tabularnewline
Total & 56\tabularnewline
\bottomrule
\end{longtable}

    \section{Getting Started}\label{getting-started}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}2}]:} \PY{c+c1}{\PYZsh{} Imports}
        \PY{k+kn}{import} \PY{n+nn}{pandas} \PY{k}{as} \PY{n+nn}{pd}
        \PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
        \PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt}
        \PY{k+kn}{import} \PY{n+nn}{csv}
        \PY{k+kn}{import} \PY{n+nn}{re}
        \PY{k+kn}{import} \PY{n+nn}{seaborn} \PY{k}{as} \PY{n+nn}{sns}
        
        \PY{c+c1}{\PYZsh{} Set some parameters}
        \PY{n}{plt}\PY{o}{.}\PY{n}{rcParams}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{figure.figsize}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{p}{(}\PY{l+m+mi}{12}\PY{p}{,} \PY{l+m+mi}{9}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{rcParams}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{font.size}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{l+m+mi}{16}
        \PY{n}{np}\PY{o}{.}\PY{n}{set\PYZus{}printoptions}\PY{p}{(}\PY{l+m+mi}{4}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}3}]:} \PY{c+c1}{\PYZsh{} We will use plot\PYZus{}3d helper function to help us visualize gradient}
        \PY{k+kn}{from} \PY{n+nn}{hw3\PYZus{}utils} \PY{k}{import} \PY{n}{plot\PYZus{}3d}
\end{Verbatim}


    \subsection{Load Data}\label{load-data}

Load the data.csv file into a pandas dataframe.\\
Note that we are reading the data directly from the URL address.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}4}]:} \PY{c+c1}{\PYZsh{} Run this cell to load our sample data}
        \PY{n}{data} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{read\PYZus{}csv}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{https://github.com/DS\PYZhy{}100/fa18/raw/gh\PYZhy{}pages/assets/datasets/hw3\PYZus{}data.csv}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{index\PYZus{}col}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}
        \PY{n}{data}\PY{o}{.}\PY{n}{head}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}4}]:}           x         y
        0 -5.000000 -7.672309
        1 -4.966555 -7.779735
        2 -4.933110 -7.995938
        3 -4.899666 -8.197059
        4 -4.866221 -8.183883
\end{Verbatim}
            
    \begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsection{1: A Simple Model}\label{a-simple-model}

Let's start by examining our data and creating a simple model that can
represent this data.

    \subsubsection{Question 1}\label{question-1}

\paragraph{Question 1a}\label{question-1a}

First, let's visualize the data in a scatter plot. After implementing
the \texttt{scatter} function below, you should see something like this:
\includegraphics{scatter.png}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}5}]:} \PY{k}{def} \PY{n+nf}{scatter}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{)}\PY{p}{:}
            \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{l+s+sd}{    Generate a scatter plot using x and y}
        
        \PY{l+s+sd}{    Keyword arguments:}
        \PY{l+s+sd}{    x \PYZhy{}\PYZhy{} the vector of values x}
        \PY{l+s+sd}{    y \PYZhy{}\PYZhy{} the vector of values y}
        \PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
            \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{8}\PY{p}{,} \PY{l+m+mi}{6}\PY{p}{)}\PY{p}{)}
            \PY{o}{.}\PY{o}{.}\PY{o}{.}
            \PY{c+c1}{\PYZsh{} YOUR CODE HERE}
            \PY{n}{plt}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{)}
            \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{x}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
            \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{y}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
            \PY{c+c1}{\PYZsh{}raise NotImplementedError()}
        
        \PY{n}{x} \PY{o}{=} \PY{n}{data}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{x}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
        \PY{n}{y} \PY{o}{=} \PY{n}{data}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{y}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
        \PY{n}{scatter}\PY{p}{(}\PY{n}{x}\PY{p}{,}\PY{n}{y}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_11_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \paragraph{Question 1b}\label{question-1b}

Describe any significant observations about the distribution of the
data. How can you describe the relationship between \(x\) and \(y\)?

    The relation is a positive relationship; as x increases, y increases as
well. However, it also seems to be increasing sinusoidally.

    \paragraph{Question 1c}\label{question-1c}

The data looks roughly linear, with some extra noise. For now, let's
assume that the data follows some underlying linear model. We define the
underlying linear model that predicts the value \(y\) using the value
\(x\) as: \(f_{\theta^*}(x) = \theta^* \cdot x\)

Since we cannot find the value of the population parameter \(\theta^*\)
exactly, we will assume that our dataset approximates our population and
use our dataset to estimate \(\theta^*\). We denote our estimation with
\(\theta\), our fitted estimation with \(\hat{\theta}\), and our model
as:

\[\Large
f_{\theta}(x) = \theta \cdot x
\]

Based on this equation, define the linear model function
\texttt{linear\_model} below to estimate \(\textbf{y}\) (the
\(y\)-values) given \(\textbf{x}\) (the \(x\)-values) and \(\theta\).
This model is similar to the model you defined in Lab 5: Modeling and
Estimation.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}6}]:} \PY{k}{def} \PY{n+nf}{linear\PYZus{}model}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{theta}\PY{p}{)}\PY{p}{:}
            \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{l+s+sd}{    Returns the estimate of y given x and theta}
        
        \PY{l+s+sd}{    Keyword arguments:}
        \PY{l+s+sd}{    x \PYZhy{}\PYZhy{} the vector of values x}
        \PY{l+s+sd}{    theta \PYZhy{}\PYZhy{} the scalar theta}
        \PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
            \PY{n}{y} \PY{o}{=} \PY{n}{theta}\PY{o}{*}\PY{n}{x}
            \PY{c+c1}{\PYZsh{} YOUR CODE HERE}
            \PY{c+c1}{\PYZsh{}raise NotImplementedError()}
            \PY{k}{return} \PY{n}{y}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}7}]:} \PY{k}{assert} \PY{n}{linear\PYZus{}model}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)} \PY{o}{==} \PY{l+m+mi}{0}
        \PY{k}{assert} \PY{n}{linear\PYZus{}model}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{)} \PY{o}{==} \PY{l+m+mi}{100}
        \PY{k}{assert} \PY{n}{np}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{n}{linear\PYZus{}model}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{5}\PY{p}{]}\PY{p}{)}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{)}\PY{p}{)} \PY{o}{==} \PY{l+m+mi}{24}
        \PY{k}{assert} \PY{n}{linear\PYZus{}model}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{l+m+mi}{7}\PY{p}{,} \PY{l+m+mi}{8}\PY{p}{]}\PY{p}{)}\PY{p}{,} \PY{l+m+mi}{4}\PY{p}{)}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)} \PY{o}{==} \PY{l+m+mi}{30}
\end{Verbatim}


    \paragraph{Question 1d}\label{question-1d}

In class, we learned that the \(L^2\) (or squared) loss function is
smooth and continuous. Let's use \(L^2\) loss to evaluate our estimate
\(\theta\), which we will use later to identify an optimal \(\theta\),
represented as \(\hat{\theta}\). Define the \(L^2\) loss function
\texttt{l2\_loss} below.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}8}]:} \PY{k}{def} \PY{n+nf}{l2\PYZus{}loss}\PY{p}{(}\PY{n}{y}\PY{p}{,} \PY{n}{y\PYZus{}hat}\PY{p}{)}\PY{p}{:}
            \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{l+s+sd}{    Returns the average l2 loss given y and y\PYZus{}hat}
        
        \PY{l+s+sd}{    Keyword arguments:}
        \PY{l+s+sd}{    y \PYZhy{}\PYZhy{} the vector of true values y}
        \PY{l+s+sd}{    y\PYZus{}hat \PYZhy{}\PYZhy{} the vector of predicted values y\PYZus{}hat}
        \PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
            \PY{k}{return} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{(}\PY{n}{y}\PY{o}{\PYZhy{}}\PY{n}{y\PYZus{}hat}\PY{p}{)}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2}\PY{p}{)}
            \PY{c+c1}{\PYZsh{} YOUR CODE HERE}
            \PY{c+c1}{\PYZsh{}raise NotImplementedError()}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}9}]:} \PY{k}{assert} \PY{n}{l2\PYZus{}loss}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)} \PY{o}{==} \PY{l+m+mi}{1}
        \PY{k}{assert} \PY{n}{l2\PYZus{}loss}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{)} \PY{o}{==} \PY{l+m+mi}{4} 
        \PY{k}{assert} \PY{n}{l2\PYZus{}loss}\PY{p}{(}\PY{l+m+mi}{5}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)} \PY{o}{==} \PY{l+m+mi}{16}
        \PY{k}{assert} \PY{n}{l2\PYZus{}loss}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{l+m+mi}{5}\PY{p}{,} \PY{l+m+mi}{6}\PY{p}{]}\PY{p}{)}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}\PY{p}{)} \PY{o}{==} \PY{l+m+mf}{20.5}
        \PY{k}{assert} \PY{n}{l2\PYZus{}loss}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{l+m+mi}{4}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{4}\PY{p}{]}\PY{p}{)}\PY{p}{)} \PY{o}{==} \PY{l+m+mf}{6.0}
\end{Verbatim}


    \paragraph{Question 1e}\label{question-1e}

First, visualize the \(L^2\) loss as a function of \(\theta\), where
several different values of \(\theta\) are given. Be sure to label your
axes properly. You plot should look something like this:
\includegraphics{l2_avg_loss.png}

What looks like the optimal value, \(\hat{\theta}\), based on the
visualization? Set \texttt{theta\_star\_guess} to the value of
\(\theta\) that appears to minimize our loss.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}10}]:} \PY{k}{def} \PY{n+nf}{visualize}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{thetas}\PY{p}{)}\PY{p}{:}
             \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
         \PY{l+s+sd}{    Plots the average l2 loss for given x, y as a function of theta.}
         \PY{l+s+sd}{    Use the functions you wrote for linear\PYZus{}model and l2\PYZus{}loss.}
         
         \PY{l+s+sd}{    Keyword arguments:}
         \PY{l+s+sd}{    x \PYZhy{}\PYZhy{} the vector of values x}
         \PY{l+s+sd}{    y \PYZhy{}\PYZhy{} the vector of values y}
         \PY{l+s+sd}{    thetas \PYZhy{}\PYZhy{} an array containing different estimates of the scalar theta}
         \PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
             \PY{n}{avg\PYZus{}loss} \PY{o}{=} \PY{p}{[}\PY{p}{]}
             
             \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{8}\PY{p}{,}\PY{l+m+mi}{6}\PY{p}{)}\PY{p}{)}
             
             \PY{k}{for} \PY{n}{theta} \PY{o+ow}{in} \PY{n}{thetas}\PY{p}{:}
                 \PY{n}{avg\PYZus{}loss} \PY{o}{+}\PY{o}{=} \PY{p}{[}\PY{n}{l2\PYZus{}loss}\PY{p}{(}\PY{n}{y}\PY{p}{,} \PY{n}{linear\PYZus{}model}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{theta}\PY{p}{)}\PY{p}{)}\PY{p}{]}
             
             \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{thetas}\PY{p}{,} \PY{n}{avg\PYZus{}loss}\PY{p}{)}
             \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Theta}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
             \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Average Loss}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
             \PY{c+c1}{\PYZsh{} YOUR CODE HERE}
             \PY{c+c1}{\PYZsh{}raise NotImplementedError()}
             
         \PY{n}{thetas} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{linspace}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{5}\PY{p}{,} \PY{l+m+mi}{70}\PY{p}{)}
         \PY{n}{visualize}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{thetas}\PY{p}{)}
         
         \PY{n}{theta\PYZus{}star\PYZus{}guess} \PY{o}{=} \PY{l+m+mf}{1.5}
         \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{y}\PY{p}{)}
         \PY{c+c1}{\PYZsh{} YOUR CODE HERE}
         \PY{c+c1}{\PYZsh{}raise NotImplementedError()}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}10}]:} -0.0017722527611192618
\end{Verbatim}
            
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_21_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}11}]:} \PY{k}{assert} \PY{n}{l2\PYZus{}loss}\PY{p}{(}\PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)} \PY{o}{==} \PY{l+m+mi}{1}
         \PY{k}{assert} \PY{n}{l2\PYZus{}loss}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{)} \PY{o}{==} \PY{l+m+mi}{100}
         \PY{k}{assert} \PY{l+m+mi}{1} \PY{o}{\PYZlt{}}\PY{o}{=} \PY{n}{theta\PYZus{}star\PYZus{}guess} \PY{o}{\PYZlt{}}\PY{o}{=} \PY{l+m+mi}{2}
\end{Verbatim}


    \begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsection{2: Fitting our Simple Model}\label{fitting-our-simple-model}

Now that we have defined a simple linear model and loss function, let's
begin working on fitting our model to the data.

\subsubsection{Question 2}\label{question-2}

Let's confirm our visual findings for optimal \(\hat{\theta}\).

\paragraph{Question 2a}\label{question-2a}

First, find the analytical solution for the optimal \(\hat{\theta}\) for
average \(L^2\) loss. Write up your solution in the cell below using
LaTex.

Hint: notice that we now have \(\textbf{x}\) and \(\textbf{y}\) instead
of \(x\) and \(y\). This means that when writing the loss function
\(L(\textbf{x}, \textbf{y}, \theta)\), you'll need to take the average
of the squared losses for each \(y_i\), \(f_\theta(x_i)\) pair. For tips
on getting started, see chapter
\href{https://www.textbook.ds100.org/ch/10/modeling_loss_functions.html}{chapter
10} of the textbook. Note that if you click "Open in DataHub", you can
access the LaTeX source code of the book chapter, which you might find
handy for typing up your work. Show your work, i.e. don't just write the
answer.

    \[L(\textbf{x}, \textbf{y}, \theta) = \frac{1}{n} \sum^{n}_{i=1}(y_i-x_i\theta)^2\]
\[\frac{\partial}{\partial \theta}L(\textbf{x}, \textbf{y}, \theta) = \frac{1}{n} \sum^{n}_{i=1}-2 x_i(y_i-x_i\theta)\]
\[\frac{\partial}{\partial \theta}L(\textbf{x}, \textbf{y}, \theta) = \frac{-2}{n} \sum^{n}_{i=1}x_i(y_i-x_i\theta)\]
\[\frac{\partial}{\partial \theta}L(\textbf{x}, \textbf{y}, \theta) = \frac{-2}{n} \sum^{n}_{i=1}(x_iy_i)+\frac{2}{n}\sum^{n}_{i=1}(x_i^2\theta)\]
To minimize \(\theta\), we must make its derivative 0.
\[0 = \frac{-1}{n} \sum^{n}_{i=1}(x_iy_i)+\frac{1}{n}\sum^{n}_{i=1}(x_i^2\theta)\]
\[\sum^{n}_{i=1}(x_iy_i) = \theta\sum^{n}_{i=1}(x_i)\]
\[\theta = \frac{\sum^{n}_{i=1}(x_iy_i)}{\sum^{n}_{i=1}(x_i^2)}\]

    \paragraph{Question 2b}\label{question-2b}

Now that we have the analytic solution for \(\hat{\theta}\), implement
the function \texttt{find\_theta} that calculates the numerical value of
\(\hat{\theta}\) based on our data \(\textbf{x}\), \(\textbf{y}\).

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}12}]:} \PY{k}{def} \PY{n+nf}{find\PYZus{}theta}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{)}\PY{p}{:}
             \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
         \PY{l+s+sd}{    Find optimal theta given x and y}
         
         \PY{l+s+sd}{    Keyword arguments:}
         \PY{l+s+sd}{    x \PYZhy{}\PYZhy{} the vector of values x}
         \PY{l+s+sd}{    y \PYZhy{}\PYZhy{} the vector of values y}
         \PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
             \PY{n}{theta\PYZus{}opt} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{n}{x}\PY{o}{*}\PY{n}{y}\PY{p}{)}\PY{o}{/}\PY{n}{np}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{n}{x}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2}\PY{p}{)}
             \PY{c+c1}{\PYZsh{} YOUR CODE HERE}
             \PY{c+c1}{\PYZsh{}raise NotImplementedError()}
             \PY{k}{return} \PY{n}{theta\PYZus{}opt}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}13}]:} \PY{n}{t\PYZus{}hat} \PY{o}{=} \PY{n}{find\PYZus{}theta}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{theta\PYZus{}opt = }\PY{l+s+si}{\PYZob{}t\PYZus{}hat\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         
         \PY{k}{assert} \PY{l+m+mf}{1.4} \PY{o}{\PYZlt{}}\PY{o}{=} \PY{n}{t\PYZus{}hat} \PY{o}{\PYZlt{}}\PY{o}{=} \PY{l+m+mf}{1.6}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
theta\_opt = 1.5502648085962225

    \end{Verbatim}

    \paragraph{Question 2c}\label{question-2c}

Now, let's plot our loss function again using the \texttt{visualize}
function. But this time, add a vertical line at the optimal value of
theta (plot the line \(x = \hat{\theta}\)). Your plot should look
something like this: \includegraphics{vertical_linear.png}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}14}]:} \PY{n}{theta\PYZus{}opt} \PY{o}{=} \PY{n}{find\PYZus{}theta}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{)}
         \PY{n}{visualize}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{thetas}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{axvline}\PY{p}{(}\PY{n}{x}\PY{o}{=}\PY{n}{theta\PYZus{}opt}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{red}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{c+c1}{\PYZsh{} YOUR CODE HERE}
         \PY{c+c1}{\PYZsh{}raise NotImplementedError()}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}14}]:} <matplotlib.lines.Line2D at 0x7fa39788b978>
\end{Verbatim}
            
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_29_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \subsubsection{Question 2d}\label{question-2d}

We now have an optimal value for \(\theta\) that minimizes our loss. In
the cell below, plot the scatter plot of the data from Question 1a (you
can reuse the \texttt{scatter} function here). But this time, add the
line \(f_{\hat{\theta}}(x) = \hat{\theta} \cdot \textbf{x}\) using the
\(\hat{\theta}\) you computed above. Your plot should look something
like this: \includegraphics{scatter_with_line.png}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}15}]:} \PY{n}{theta\PYZus{}opt} \PY{o}{=} \PY{n}{find\PYZus{}theta}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{)}
         \PY{n}{scatter}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{x}\PY{o}{*}\PY{n}{theta\PYZus{}opt}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{red}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{c+c1}{\PYZsh{} YOUR CODE HERE}
         \PY{c+c1}{\PYZsh{}raise NotImplementedError()}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}15}]:} [<matplotlib.lines.Line2D at 0x7fa3977fc438>]
\end{Verbatim}
            
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_31_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \paragraph{Question 2e}\label{question-2e}

Great! It looks like our estimator \(f_{\hat{\theta}}(x)\) is able to
capture a lot of the data with a single parameter \(\theta\). Now let's
try to remove the linear portion of our model from the data to see if we
missed anything.

The remaining data is known as the residual,
\(\textbf{r}=\textbf{y}-\hat{\theta} \cdot \textbf{x}\). Below, write a
function to find the residual and plot the residuals corresponding to
\(x\) in a scatter plot. Plot a horizontal line at \(y=0\) to assist
visualization.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}16}]:} \PY{k}{def} \PY{n+nf}{visualize\PYZus{}residual}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{)}\PY{p}{:}
             \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
         \PY{l+s+sd}{    Plot a scatter plot of the residuals, the remaining }
         \PY{l+s+sd}{    values after removing the linear model from our data.}
         
         \PY{l+s+sd}{    Keyword arguments:}
         \PY{l+s+sd}{    x \PYZhy{}\PYZhy{} the vector of values x}
         \PY{l+s+sd}{    y \PYZhy{}\PYZhy{} the vector of values y}
         \PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
             \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{o}{\PYZhy{}}\PY{n}{find\PYZus{}theta}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{)}\PY{o}{*}\PY{n}{x}\PY{p}{)}
             \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{x}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
             \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Residuals}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
             \PY{n}{plt}\PY{o}{.}\PY{n}{axhline}\PY{p}{(}\PY{n}{y}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{red}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
             \PY{c+c1}{\PYZsh{} YOUR CODE HERE}
             \PY{c+c1}{\PYZsh{}raise NotImplementedError()}
         
         \PY{n}{visualize\PYZus{}residual}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_33_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \paragraph{Question 2f}\label{question-2f}

What does the residual look like? Do you notice a relationship between
\(x\) and \(r\)?

    The residuals plot looks periodic and sinusoidal. The relationship
between x and r seems sinusoidal.

    \begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsection{3: Increasing Model
Complexity}\label{increasing-model-complexity}

It looks like the remaining data is sinusoidal, meaning our original
data follows a linear function and a sinusoidal function. Let's define a
new model to address this discovery and find optimal parameters to best
fit the data:

\[\Large
f_\boldsymbol\theta(x) = \theta_1x + sin(\theta_2x)
\]

Now, our model is parameterized by both \(\theta_1\) and \(\theta_2\),
or composed together, \(\boldsymbol{\theta}\).

Note that a generalized sine function \(a\sin(bx+c)\) has three
parameters: amplitude scaling parameter \(a\), frequency parameter \(b\)
and phase shifting parameter \(c\). Looking at the residual plot above,
it looks like the residual is zero at x = 0, and the residual swings
between -1 and 1. Thus, it seems reasonable to effectively set the
scaling and phase shifting parameter (\(a\) and \(c\) in this case) to 1
and 0 respectively. While we could try to fit \(a\) and \(c\), we're
unlikely to get much benefit. When you're done with the homework, you
can try adding \(a\) and \(c\) to our model and fitting these values to
see if you can get a better loss.

    \paragraph{Question 3a}\label{question-3a}

As in Question 1, fill in the \texttt{sin\_model} function that predicts
\(\textbf{y}\) (the \(y\)-values) using \(\textbf{x}\) (the
\(x\)-values), but this time based on our new equation.

\emph{Hint:} Try to do this without using for loops. The \texttt{np.sin}
function may help you.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}17}]:} \PY{k}{def} \PY{n+nf}{sin\PYZus{}model}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{theta\PYZus{}1}\PY{p}{,} \PY{n}{theta\PYZus{}2}\PY{p}{)}\PY{p}{:}
             \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
         \PY{l+s+sd}{    Predict the estimate of y given x, theta\PYZus{}1, theta\PYZus{}2}
         
         \PY{l+s+sd}{    Keyword arguments:}
         \PY{l+s+sd}{    x \PYZhy{}\PYZhy{} the vector of values x}
         \PY{l+s+sd}{    theta\PYZus{}1 \PYZhy{}\PYZhy{} the scalar value theta\PYZus{}1}
         \PY{l+s+sd}{    theta\PYZus{}2 \PYZhy{}\PYZhy{} the scalar value theta\PYZus{}2}
         \PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
             \PY{n}{y} \PY{o}{=} \PY{n}{theta\PYZus{}1}\PY{o}{*}\PY{n}{x} \PY{o}{+} \PY{n}{np}\PY{o}{.}\PY{n}{sin}\PY{p}{(}\PY{n}{theta\PYZus{}2}\PY{o}{*}\PY{n}{x}\PY{p}{)}
             \PY{c+c1}{\PYZsh{} YOUR CODE HERE}
             \PY{c+c1}{\PYZsh{}raise NotImplementedError()}
             \PY{k}{return} \PY{n}{y}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}18}]:} \PY{k}{assert} \PY{n}{np}\PY{o}{.}\PY{n}{isclose}\PY{p}{(}\PY{n}{sin\PYZus{}model}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{pi}\PY{p}{)}\PY{p}{,} \PY{l+m+mf}{1.0000000000000002}\PY{p}{)}
         \PY{c+c1}{\PYZsh{} Check that we accept x as arrays}
         \PY{k}{assert} \PY{n+nb}{len}\PY{p}{(}\PY{n}{sin\PYZus{}model}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)} \PY{o}{\PYZgt{}} \PY{l+m+mi}{1}
\end{Verbatim}


    \paragraph{Question 3b}\label{question-3b}

Use the average \(L^2\) loss to compute
\(\frac{\partial L }{\partial \theta_1}, \frac{\partial L }{\partial \theta_2}\).

First, we will use LaTex to write
\(L(\textbf{x}, \textbf{y}, \theta_1, \theta_2)\),
\(\frac{\partial L }{\partial \theta_1}\), and
\(\frac{\partial L }{\partial \theta_2}\) given \(\textbf{x}\),
\(\textbf{y}\), \(\boldsymbol{\theta}\).

You don't need to write out the full derivation. Just the final
expression is fine.

    \[\frac{\partial}{\partial \theta_1}L(\textbf{x}, \textbf{y}, \theta_1, \theta_2) = \frac{-2}{n} \sum^{n}_{i=1}x_i(y_i-\theta_1x_i-\sin(\theta_2x_i))\]
\[\frac{\partial}{\partial \theta_2}L(\textbf{x}, \textbf{y}, \theta_1, \theta_2) = \frac{-2}{n} \sum^{n}_{i=1} x_i\cos(\theta_2x_i)(y_i-\theta_1x_i-\sin(\theta_2x_i))\]

    \paragraph{Question 3c}\label{question-3c}

Now, implement the functions \texttt{dt1} and \texttt{dt2}, which should
compute \(\frac{\partial L }{\partial \theta_1}\) and
\(\frac{\partial L }{\partial \theta_2}\) respectively. Use the formulas
you wrote for \(\frac{\partial L }{\partial \theta_1}\) and
\(\frac{\partial L }{\partial \theta_2}\) in the previous exercise. In
the functions below, the parameter \texttt{theta} is a vector that looks
like \(( \theta_1, \theta_2 )\).

Note: To keep your code a bit more concise, be aware that
\texttt{np.mean} does the same thing as \texttt{np.sum} divided by the
length of the numpy array.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}19}]:} \PY{k}{def} \PY{n+nf}{dt1}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{theta}\PY{p}{)}\PY{p}{:}
             \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
         \PY{l+s+sd}{    Compute the numerical value of the partial of l2 loss with respect to theta\PYZus{}1}
         
         \PY{l+s+sd}{    Keyword arguments:}
         \PY{l+s+sd}{    x \PYZhy{}\PYZhy{} the vector of all x values}
         \PY{l+s+sd}{    y \PYZhy{}\PYZhy{} the vector of all y values}
         \PY{l+s+sd}{    theta \PYZhy{}\PYZhy{} the vector of values theta}
         \PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
             \PY{k}{return} \PY{o}{\PYZhy{}}\PY{l+m+mi}{2}\PY{o}{*}\PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{x}\PY{o}{*}\PY{p}{(}\PY{n}{y}\PY{o}{\PYZhy{}}\PY{n}{theta}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{*}\PY{n}{x}\PY{o}{\PYZhy{}}\PY{n}{np}\PY{o}{.}\PY{n}{sin}\PY{p}{(}\PY{n}{theta}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{*}\PY{n}{x}\PY{p}{)}\PY{p}{)}\PY{p}{)}
             \PY{c+c1}{\PYZsh{} YOUR CODE HERE}
             \PY{c+c1}{\PYZsh{}raise NotImplementedError()}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}20}]:} \PY{k}{def} \PY{n+nf}{dt2}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{theta}\PY{p}{)}\PY{p}{:}
             \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
         \PY{l+s+sd}{    Compute the numerical value of the partial of l2 loss with respect to theta\PYZus{}2}
         
         \PY{l+s+sd}{    Keyword arguments:}
         \PY{l+s+sd}{    x \PYZhy{}\PYZhy{} the vector of all x values}
         \PY{l+s+sd}{    y \PYZhy{}\PYZhy{} the vector of all y values}
         \PY{l+s+sd}{    theta \PYZhy{}\PYZhy{} the vector of values theta}
         \PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
             \PY{k}{return} \PY{o}{\PYZhy{}}\PY{l+m+mi}{2}\PY{o}{*}\PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{x}\PY{o}{*}\PY{n}{np}\PY{o}{.}\PY{n}{cos}\PY{p}{(}\PY{n}{theta}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{*}\PY{n}{x}\PY{p}{)}\PY{o}{*}\PY{p}{(}\PY{n}{y}\PY{o}{\PYZhy{}}\PY{n}{theta}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{*}\PY{n}{x}\PY{o}{\PYZhy{}}\PY{n}{np}\PY{o}{.}\PY{n}{sin}\PY{p}{(}\PY{n}{theta}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{*}\PY{n}{x}\PY{p}{)}\PY{p}{)}\PY{p}{)}
             \PY{c+c1}{\PYZsh{} YOUR CODE HERE}
             \PY{c+c1}{\PYZsh{}raise NotImplementedError()}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}21}]:} \PY{c+c1}{\PYZsh{} This function calls dt1 and dt2 and returns the gradient dt. It is already implemented for you.}
         \PY{k}{def} \PY{n+nf}{dt}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{theta}\PY{p}{)}\PY{p}{:}
             \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
         \PY{l+s+sd}{    Returns the gradient of l2 loss with respect to vector theta}
         
         \PY{l+s+sd}{    Keyword arguments:}
         \PY{l+s+sd}{    x \PYZhy{}\PYZhy{} the vector of values x}
         \PY{l+s+sd}{    y \PYZhy{}\PYZhy{} the vector of values y}
         \PY{l+s+sd}{    theta \PYZhy{}\PYZhy{} the vector of values theta}
         \PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
             \PY{k}{return} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{n}{dt1}\PY{p}{(}\PY{n}{x}\PY{p}{,}\PY{n}{y}\PY{p}{,}\PY{n}{theta}\PY{p}{)}\PY{p}{,} \PY{n}{dt2}\PY{p}{(}\PY{n}{x}\PY{p}{,}\PY{n}{y}\PY{p}{,}\PY{n}{theta}\PY{p}{)}\PY{p}{]}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}22}]:} \PY{k}{assert} \PY{n}{np}\PY{o}{.}\PY{n}{isclose}\PY{p}{(}\PY{n}{dt1}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{pi}\PY{p}{]}\PY{p}{)}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mf}{25.376660670924529}\PY{p}{)}
         \PY{k}{assert} \PY{n}{np}\PY{o}{.}\PY{n}{isclose}\PY{p}{(}\PY{n}{dt2}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{pi}\PY{p}{]}\PY{p}{)}\PY{p}{,} \PY{l+m+mf}{1.9427210155296564}\PY{p}{)}
\end{Verbatim}


    \begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsection{4: Gradient Descent}\label{gradient-descent}

Now try to solve for the optimal \(\hat{\theta}\) analytically...

\textbf{Just kidding!}

You can try but we don't recommend it. When finding an analytic solution
becomes difficult or impossible, we resort to alternative optimization
methods for finding an approximate solution.

\subsubsection{Question 4}\label{question-4}

So let's try implementing a numerical optimization method: gradient
descent!

\paragraph{Question 4a}\label{question-4a}

Implement the \texttt{grad\_desc} function that performs gradient
descent for a finite number of iterations. This function takes in an
array for \(\textbf{x}\) (\texttt{x}), an array for \(\textbf{y}\)
(\texttt{y}), and an initial value for \(\theta\) (\texttt{theta}).
\texttt{alpha} will be the learning rate (or step size, whichever term
you prefer). In this part, we'll use a static learning rate that is the
same at every time step.

At each time step, use the gradient and \texttt{alpha} to update your
current \texttt{theta}. Also at each time step, be sure to save the
current \texttt{theta} in \texttt{theta\_history}, along with the
\(L^2\) loss (computed with the current \texttt{theta}) in
\texttt{loss\_history}.

Hints: - Write out the gradient update equation (1 step). What variables
will you need for each gradient update? Of these variables, which ones
do you already have, and which ones will you need to recompute at each
time step? - You may need a loop here to update \texttt{theta} several
times - Recall that the gradient descent update function follows the
form: \[\large
\boldsymbol\theta^{(t+1)} \leftarrow \boldsymbol\theta^{(t)} - \alpha \left(\nabla_\boldsymbol\theta \mathbf{L}(\textbf{x}, \textbf{y}, \boldsymbol\theta^{(t)}) \right)
\]

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}23}]:} \PY{c+c1}{\PYZsh{} Run me}
         \PY{k}{def} \PY{n+nf}{init\PYZus{}t}\PY{p}{(}\PY{p}{)}\PY{p}{:}
             \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Creates an initial theta [0, 0] of shape (2,) as a starting point for gradient descent\PYZdq{}\PYZdq{}\PYZdq{}}
             \PY{k}{return} \PY{n}{np}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}24}]:} \PY{k}{def} \PY{n+nf}{grad\PYZus{}desc}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{theta}\PY{p}{,} \PY{n}{num\PYZus{}iter}\PY{o}{=}\PY{l+m+mi}{20}\PY{p}{,} \PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{0.1}\PY{p}{)}\PY{p}{:}
             \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
         \PY{l+s+sd}{    Run gradient descent update for a finite number of iterations and static learning rate}
         
         \PY{l+s+sd}{    Keyword arguments:}
         \PY{l+s+sd}{    x \PYZhy{}\PYZhy{} the vector of values x}
         \PY{l+s+sd}{    y \PYZhy{}\PYZhy{} the vector of values y}
         \PY{l+s+sd}{    theta \PYZhy{}\PYZhy{} the vector of values theta to use at first iteration}
         \PY{l+s+sd}{    num\PYZus{}iter \PYZhy{}\PYZhy{} the max number of iterations}
         \PY{l+s+sd}{    alpha \PYZhy{}\PYZhy{} the learning rate (also called the step size)}
         \PY{l+s+sd}{    }
         \PY{l+s+sd}{    Return:}
         \PY{l+s+sd}{    theta \PYZhy{}\PYZhy{} the optimal value of theta after num\PYZus{}iter of gradient descent}
         \PY{l+s+sd}{    theta\PYZus{}history \PYZhy{}\PYZhy{} the series of theta values over each iteration of gradient descent}
         \PY{l+s+sd}{    loss\PYZus{}history \PYZhy{}\PYZhy{} the series of loss values over each iteration of gradient descent}
         \PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
             \PY{n}{theta\PYZus{}history} \PY{o}{=} \PY{p}{[}\PY{p}{]}
             \PY{n}{loss\PYZus{}history} \PY{o}{=} \PY{p}{[}\PY{p}{]}
             \PY{n}{theta1} \PY{o}{=} \PY{n}{theta}\PY{p}{[}\PY{p}{:}\PY{p}{]}
             
             \PY{k}{for} \PY{n}{\PYZus{}} \PY{o+ow}{in} \PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{n}{num\PYZus{}iter}\PY{p}{)}\PY{p}{:}
                 \PY{n}{loss} \PY{o}{=} \PY{n}{l2\PYZus{}loss}\PY{p}{(}\PY{n}{y}\PY{p}{,} \PY{n}{sin\PYZus{}model}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{theta1}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{theta1}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}\PY{p}{)}
                 \PY{n}{theta1} \PY{o}{+}\PY{o}{=} \PY{o}{\PYZhy{}}\PY{n}{alpha}\PY{o}{*}\PY{p}{(}\PY{n}{dt}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{theta1}\PY{p}{[}\PY{p}{:}\PY{p}{]}\PY{p}{)}\PY{p}{)}
                 \PY{n}{theta\PYZus{}history} \PY{o}{+}\PY{o}{=} \PY{p}{[}\PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{theta1}\PY{p}{)}\PY{p}{]}
                 \PY{n}{loss\PYZus{}history} \PY{o}{+}\PY{o}{=} \PY{p}{[}\PY{n}{loss}\PY{p}{]}
             \PY{c+c1}{\PYZsh{} YOUR CODE HERE}
             \PY{c+c1}{\PYZsh{}raise NotImplementedError()}
             \PY{k}{return} \PY{n}{theta1}\PY{p}{,} \PY{n}{theta\PYZus{}history}\PY{p}{,} \PY{n}{loss\PYZus{}history}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}25}]:} \PY{n}{t} \PY{o}{=} \PY{n}{init\PYZus{}t}\PY{p}{(}\PY{p}{)}
         \PY{n}{t\PYZus{}est}\PY{p}{,} \PY{n}{ts}\PY{p}{,} \PY{n}{loss} \PY{o}{=} \PY{n}{grad\PYZus{}desc}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{t}\PY{p}{,} \PY{n}{num\PYZus{}iter}\PY{o}{=}\PY{l+m+mi}{20}\PY{p}{,} \PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{0.1}\PY{p}{)}
         
         \PY{k}{assert} \PY{n+nb}{len}\PY{p}{(}\PY{n}{ts}\PY{p}{)} \PY{o}{==} \PY{n+nb}{len}\PY{p}{(}\PY{n}{loss}\PY{p}{)} \PY{o}{==} \PY{l+m+mi}{20} \PY{c+c1}{\PYZsh{} theta history and loss history are 20 items in them}
         \PY{k}{assert} \PY{n}{ts}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{shape} \PY{o}{==} \PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,}\PY{p}{)} \PY{c+c1}{\PYZsh{} theta history contains theta values}
         \PY{k}{assert} \PY{n}{np}\PY{o}{.}\PY{n}{isscalar}\PY{p}{(}\PY{n}{loss}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)} \PY{c+c1}{\PYZsh{} loss history is a list of scalar values, not vector}
         
         \PY{k}{assert} \PY{n}{loss}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]} \PY{o}{\PYZhy{}} \PY{n}{loss}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]} \PY{o}{\PYZgt{}} \PY{l+m+mi}{0} \PY{c+c1}{\PYZsh{} loss is decreasing}
         
         \PY{k}{assert} \PY{n}{np}\PY{o}{.}\PY{n}{allclose}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{n}{t\PYZus{}est}\PY{p}{)}\PY{p}{,} \PY{l+m+mf}{4.5}\PY{p}{,} \PY{n}{atol}\PY{o}{=}\PY{l+m+mf}{2e\PYZhy{}1}\PY{p}{)}  \PY{c+c1}{\PYZsh{} theta\PYZus{}est should be close to our value}
\end{Verbatim}


    \paragraph{Question 4b}\label{question-4b}

Now, let's try using a decaying learning rate. Implement
\texttt{grad\_desc\_decay} below, which performs gradient descent with a
learning rate that decreases slightly with each time step. You should be
able to copy most of your work from the previous part, but you'll need
to tweak how you update \texttt{theta} at each time step.

By decaying learning rate, we mean instead of just a number \(\alpha\),
the learning should be now \(\frac{\alpha}{i+1}\) where \(i\) is the
current number of iteration. (Why do we need to add '+ 1' in the
denominator?)

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}26}]:} \PY{k}{def} \PY{n+nf}{grad\PYZus{}desc\PYZus{}decay}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{theta}\PY{p}{,} \PY{n}{num\PYZus{}iter}\PY{o}{=}\PY{l+m+mi}{20}\PY{p}{,} \PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{0.1}\PY{p}{)}\PY{p}{:}
             \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
         \PY{l+s+sd}{    Run gradient descent update for a finite number of iterations and decaying learning rate}
         
         \PY{l+s+sd}{    Keyword arguments:}
         \PY{l+s+sd}{    x \PYZhy{}\PYZhy{} the vector of values x}
         \PY{l+s+sd}{    y \PYZhy{}\PYZhy{} the vector of values y}
         \PY{l+s+sd}{    theta \PYZhy{}\PYZhy{} the vector of values theta}
         \PY{l+s+sd}{    num\PYZus{}iter \PYZhy{}\PYZhy{} the max number of iterations}
         \PY{l+s+sd}{    alpha \PYZhy{}\PYZhy{} the learning rate}
         \PY{l+s+sd}{    }
         \PY{l+s+sd}{    Return:}
         \PY{l+s+sd}{    theta \PYZhy{}\PYZhy{} the optimal value of theta after num\PYZus{}iter of gradient descent}
         \PY{l+s+sd}{    theta\PYZus{}history \PYZhy{}\PYZhy{} the series of theta values over each iteration of gradient descent}
         \PY{l+s+sd}{    loss\PYZus{}history \PYZhy{}\PYZhy{} the series of loss values over each iteration of gradient descent}
         \PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
             \PY{n}{theta\PYZus{}history} \PY{o}{=} \PY{p}{[}\PY{p}{]}
             \PY{n}{loss\PYZus{}history} \PY{o}{=} \PY{p}{[}\PY{p}{]}
             \PY{n}{theta1} \PY{o}{=} \PY{n}{theta}\PY{p}{[}\PY{p}{:}\PY{p}{]}
             
             \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{n}{num\PYZus{}iter}\PY{p}{)}\PY{p}{:}
                 \PY{n}{loss} \PY{o}{=} \PY{n}{l2\PYZus{}loss}\PY{p}{(}\PY{n}{y}\PY{p}{,} \PY{n}{sin\PYZus{}model}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{theta1}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{theta1}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}\PY{p}{)}
                 \PY{n}{theta1} \PY{o}{+}\PY{o}{=} \PY{o}{\PYZhy{}}\PY{n}{alpha}\PY{o}{*}\PY{p}{(}\PY{n}{dt}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{theta1}\PY{p}{[}\PY{p}{:}\PY{p}{]}\PY{p}{)}\PY{p}{)}\PY{o}{/}\PY{p}{(}\PY{n}{i}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{)}
                 \PY{n}{theta\PYZus{}history} \PY{o}{+}\PY{o}{=} \PY{p}{[}\PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{theta1}\PY{p}{)}\PY{p}{]}
                 \PY{n}{loss\PYZus{}history} \PY{o}{+}\PY{o}{=} \PY{p}{[}\PY{n}{loss}\PY{p}{]}
             \PY{c+c1}{\PYZsh{} YOUR CODE HERE}
             \PY{c+c1}{\PYZsh{}raise NotImplementedError()}
             \PY{k}{return} \PY{n}{theta1}\PY{p}{,} \PY{n}{theta\PYZus{}history}\PY{p}{,} \PY{n}{loss\PYZus{}history}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}27}]:} \PY{n}{t} \PY{o}{=} \PY{n}{init\PYZus{}t}\PY{p}{(}\PY{p}{)}
         \PY{n}{t\PYZus{}est\PYZus{}decay}\PY{p}{,} \PY{n}{ts\PYZus{}decay}\PY{p}{,} \PY{n}{loss\PYZus{}decay} \PY{o}{=} \PY{n}{grad\PYZus{}desc\PYZus{}decay}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{t}\PY{p}{,} \PY{n}{num\PYZus{}iter}\PY{o}{=}\PY{l+m+mi}{20}\PY{p}{,} \PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{0.1}\PY{p}{)}
         
         \PY{k}{assert} \PY{n+nb}{len}\PY{p}{(}\PY{n}{ts\PYZus{}decay}\PY{p}{)} \PY{o}{==} \PY{n+nb}{len}\PY{p}{(}\PY{n}{loss\PYZus{}decay}\PY{p}{)} \PY{o}{==} \PY{l+m+mi}{20} \PY{c+c1}{\PYZsh{} theta history and loss history are 20 items in them}
         \PY{k}{assert} \PY{n}{ts\PYZus{}decay}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{shape} \PY{o}{==} \PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,}\PY{p}{)} \PY{c+c1}{\PYZsh{} theta history contains theta values}
         \PY{k}{assert} \PY{n}{np}\PY{o}{.}\PY{n}{isscalar}\PY{p}{(}\PY{n}{loss}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)} \PY{c+c1}{\PYZsh{} loss history should be a list of values, not vector}
         
         \PY{k}{assert} \PY{n}{loss\PYZus{}decay}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]} \PY{o}{\PYZhy{}} \PY{n}{loss\PYZus{}decay}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]} \PY{o}{\PYZgt{}} \PY{l+m+mi}{0} \PY{c+c1}{\PYZsh{} loss is decreasing}
         
         \PY{k}{assert} \PY{n}{np}\PY{o}{.}\PY{n}{allclose}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{n}{t\PYZus{}est\PYZus{}decay}\PY{p}{)}\PY{p}{,} \PY{l+m+mf}{4.5}\PY{p}{,} \PY{n}{atol}\PY{o}{=}\PY{l+m+mf}{2e\PYZhy{}1}\PY{p}{)}  \PY{c+c1}{\PYZsh{} theta\PYZus{}est should be close to our value}
\end{Verbatim}


    \paragraph{Question 4c}\label{question-4c}

Let's visually inspect our results of running gradient descent to
optimize \(\boldsymbol\theta\). Plot our \(x\)-values with our model's
predicted \(y\)-values over the original scatter plot. Did gradient
descent successfully optimize \(\boldsymbol\theta\)?

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}28}]:} \PY{c+c1}{\PYZsh{} Run me}
         \PY{n}{t} \PY{o}{=} \PY{n}{init\PYZus{}t}\PY{p}{(}\PY{p}{)}
         \PY{n}{t\PYZus{}est}\PY{p}{,} \PY{n}{ts}\PY{p}{,} \PY{n}{loss} \PY{o}{=} \PY{n}{grad\PYZus{}desc}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{t}\PY{p}{)}
         
         \PY{n}{t} \PY{o}{=} \PY{n}{init\PYZus{}t}\PY{p}{(}\PY{p}{)}
         \PY{n}{t\PYZus{}est\PYZus{}decay}\PY{p}{,} \PY{n}{ts\PYZus{}decay}\PY{p}{,} \PY{n}{loss\PYZus{}decay} \PY{o}{=} \PY{n}{grad\PYZus{}desc\PYZus{}decay}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{t}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}29}]:} \PY{n}{y\PYZus{}pred} \PY{o}{=} \PY{n}{sin\PYZus{}model}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{t\PYZus{}est}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{t\PYZus{}est}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
         
         \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y\PYZus{}pred}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Model}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{0.5}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Observation}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{gold}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{)}\PY{p}{;}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_56_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Yes. Though we are unable to perfectly optimize theta (since we did not
analytically solve for theta), the gradient decent approach optimized
theta successfully to a practical level.

    \paragraph{Question 4d}\label{question-4d}

Let's compare our two gradient descent methods and see how they differ.
Plot the loss values over each iteration of gradient descent for both
static learning rate and decaying learning rate.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}30}]:} \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{loss}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{static learning rate}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)} \PY{c+c1}{\PYZsh{} Plot of loss history for static learning rate}
         \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{loss\PYZus{}decay}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{decaying learning rate}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)} \PY{c+c1}{\PYZsh{} Plot of loss history for decaying learning rate}
         \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Number of steps}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{L2 Loss}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{)}
         \PY{c+c1}{\PYZsh{} YOUR CODE HERE}
         \PY{c+c1}{\PYZsh{}raise NotImplementedError()}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}30}]:} <matplotlib.legend.Legend at 0x7fa397679e80>
\end{Verbatim}
            
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_59_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \paragraph{Question 4e}\label{question-4e}

Compare and contrast the performance of the two gradient descent
methods. Which method begins to converge more quickly?

    The static learning rate resulted in a loss curve that converged later
than the decaying learning rate. This is probably because the static
learning rate caused the value of theta to oscillate about the optimal
theta vector, where the the decaying learning rate oscillated less, and
thus converged quicker.

    \begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsection{5: Visualizing Loss}\label{visualizing-loss}

\subsubsection{Question 5:}\label{question-5}

Let's visualize our loss functions and gain some insight as to how
gradient descent and stochastic gradient descent are optimizing our
model parameters.

\paragraph{Question 5a:}\label{question-5a}

In the previous plot is about the loss decrease over time, but what
exactly is path the theta value? Run the following three cells.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}31}]:} \PY{c+c1}{\PYZsh{} Run me}
         \PY{n}{ts} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{ts}\PY{p}{)}\PY{o}{.}\PY{n}{squeeze}\PY{p}{(}\PY{p}{)}
         \PY{n}{ts\PYZus{}decay} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{ts\PYZus{}decay}\PY{p}{)}\PY{o}{.}\PY{n}{squeeze}\PY{p}{(}\PY{p}{)}
         \PY{n}{loss} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{loss}\PY{p}{)}
         \PY{n}{loss\PYZus{}decay} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{loss\PYZus{}decay}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}32}]:} \PY{c+c1}{\PYZsh{} Run me to see a 3D plot (gradient descent with static alpha)}
         \PY{n}{plot\PYZus{}3d}\PY{p}{(}\PY{n}{ts}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{ts}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{loss}\PY{p}{,} \PY{n}{l2\PYZus{}loss}\PY{p}{,} \PY{n}{sin\PYZus{}model}\PY{p}{,} \PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{)}
\end{Verbatim}


    
    
    
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}33}]:} \PY{c+c1}{\PYZsh{} Run me to see another 3D plot (gradient descent with decaying alpha)}
         \PY{n}{plot\PYZus{}3d}\PY{p}{(}\PY{n}{ts\PYZus{}decay}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{ts\PYZus{}decay}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{loss\PYZus{}decay}\PY{p}{,} \PY{n}{l2\PYZus{}loss}\PY{p}{,} \PY{n}{sin\PYZus{}model}\PY{p}{,} \PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{)}
\end{Verbatim}


    
    
    
    
    In the following cell, write 1-2 sentences about the differences between
using a static learning rate and a learning rate with decay for gradient
descent. Use the loss history plot as well as the two 3D visualization
to support your answer.

    For the static learning rate, as time progresses, the theta vector
oscillates about the optimal theta, because the learning rate is static
and thus remains large. In contrast, for the decaying learning rate, as
time progresses, the theta vector steadily approaches the optimal theta,
because the learning rate decays and thus does not "overshoot" the
optimal theta.

    \paragraph{Question 5b:}\label{question-5b}

Another common way of visualizing 3D dynamics is with a \emph{contour}
plot.

Please refer to this notebook when you are working on the next question:
Please refer to this notebook when you are working on the next question:
http://www.ds100.org/fa18/assets/lectures/lec09/09-Models-and-Estimation-II.html.
Search the page for \texttt{go.Contour}.

In next question, fill in the necessary part to create a contour plot.
Then run the following cells.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}34}]:} \PY{c+c1}{\PYZsh{}\PYZsh{} Run me}
         \PY{k+kn}{import} \PY{n+nn}{plotly}
         \PY{k+kn}{import} \PY{n+nn}{plotly}\PY{n+nn}{.}\PY{n+nn}{graph\PYZus{}objs} \PY{k}{as} \PY{n+nn}{go}
         \PY{n}{plotly}\PY{o}{.}\PY{n}{offline}\PY{o}{.}\PY{n}{init\PYZus{}notebook\PYZus{}mode}\PY{p}{(}\PY{n}{connected}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
\end{Verbatim}


    
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}35}]:} \PY{k}{def} \PY{n+nf}{contour\PYZus{}plot}\PY{p}{(}\PY{n}{title}\PY{p}{,} \PY{n}{theta\PYZus{}history}\PY{p}{,} \PY{n}{loss\PYZus{}function}\PY{p}{,} \PY{n}{model}\PY{p}{,} \PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{)}\PY{p}{:}
             \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
         \PY{l+s+sd}{    The function takes the following as argument:}
         \PY{l+s+sd}{        theta\PYZus{}history: a (N, 2) array of theta history}
         \PY{l+s+sd}{        loss: a list or array of loss value}
         \PY{l+s+sd}{        loss\PYZus{}function: for example, l2\PYZus{}loss}
         \PY{l+s+sd}{        model: for example, sin\PYZus{}model}
         \PY{l+s+sd}{        x: the original x input}
         \PY{l+s+sd}{        y: the original y output}
         \PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
             \PY{n}{theta\PYZus{}1\PYZus{}series} \PY{o}{=} \PY{n}{theta\PYZus{}history}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{]} \PY{c+c1}{\PYZsh{} a list or array of theta\PYZus{}1 value}
             \PY{n}{theta\PYZus{}2\PYZus{}series} \PY{o}{=} \PY{n}{theta\PYZus{}history}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{]} \PY{c+c1}{\PYZsh{} a list or array of theta\PYZus{}2 value}
             
             \PY{c+c1}{\PYZsh{} Create trace of theta point}
             \PY{c+c1}{\PYZsh{} Uncomment the following lines and fill in the TODOS}
             \PY{n}{thata\PYZus{}points} \PY{o}{=} \PY{n}{go}\PY{o}{.}\PY{n}{Scatter}\PY{p}{(}\PY{n}{name}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Theta Values}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} 
                                        \PY{n}{x}\PY{o}{=}\PY{n}{theta\PYZus{}1\PYZus{}series}\PY{p}{,} \PY{c+c1}{\PYZsh{}TODO}
                                        \PY{n}{y}\PY{o}{=}\PY{n}{theta\PYZus{}2\PYZus{}series}\PY{p}{,} \PY{c+c1}{\PYZsh{}TODO}
                                        \PY{n}{mode}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{lines+markers}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         
             \PY{c+c1}{\PYZsh{}\PYZsh{} In the following block of code, we generate the z value}
             \PY{c+c1}{\PYZsh{}\PYZsh{} across a 2D grid}
             \PY{n}{t1\PYZus{}s} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{linspace}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{min}\PY{p}{(}\PY{n}{theta\PYZus{}1\PYZus{}series}\PY{p}{)} \PY{o}{\PYZhy{}} \PY{l+m+mf}{0.1}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{max}\PY{p}{(}\PY{n}{theta\PYZus{}1\PYZus{}series}\PY{p}{)} \PY{o}{+} \PY{l+m+mf}{0.1}\PY{p}{)}
             \PY{n}{t2\PYZus{}s} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{linspace}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{min}\PY{p}{(}\PY{n}{theta\PYZus{}2\PYZus{}series}\PY{p}{)} \PY{o}{\PYZhy{}} \PY{l+m+mf}{0.1}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{max}\PY{p}{(}\PY{n}{theta\PYZus{}2\PYZus{}series}\PY{p}{)} \PY{o}{+} \PY{l+m+mf}{0.1}\PY{p}{)}
         
             \PY{n}{x\PYZus{}s}\PY{p}{,} \PY{n}{y\PYZus{}s} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{meshgrid}\PY{p}{(}\PY{n}{t1\PYZus{}s}\PY{p}{,} \PY{n}{t2\PYZus{}s}\PY{p}{)}
             \PY{n}{data} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{stack}\PY{p}{(}\PY{p}{[}\PY{n}{x\PYZus{}s}\PY{o}{.}\PY{n}{flatten}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{y\PYZus{}s}\PY{o}{.}\PY{n}{flatten}\PY{p}{(}\PY{p}{)}\PY{p}{]}\PY{p}{)}\PY{o}{.}\PY{n}{T}
             \PY{n}{ls} \PY{o}{=} \PY{p}{[}\PY{p}{]}
             \PY{k}{for} \PY{n}{t1}\PY{p}{,} \PY{n}{t2} \PY{o+ow}{in} \PY{n}{data}\PY{p}{:}
                 \PY{n}{l} \PY{o}{=} \PY{n}{loss\PYZus{}function}\PY{p}{(}\PY{n}{model}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{t1}\PY{p}{,} \PY{n}{t2}\PY{p}{)}\PY{p}{,} \PY{n}{y}\PY{p}{)}
                 \PY{n}{ls}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{l}\PY{p}{)}
             \PY{n}{z} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{ls}\PY{p}{)}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{l+m+mi}{50}\PY{p}{,} \PY{l+m+mi}{50}\PY{p}{)}
             
             \PY{c+c1}{\PYZsh{} Create the contour }
             \PY{c+c1}{\PYZsh{} Uncomment the following lines and fill in the TODOS}
             \PY{n}{lr\PYZus{}loss\PYZus{}contours} \PY{o}{=} \PY{n}{go}\PY{o}{.}\PY{n}{Contour}\PY{p}{(}\PY{n}{x}\PY{o}{=}\PY{n}{t1\PYZus{}s}\PY{p}{,} \PY{c+c1}{\PYZsh{}TODO}
                                            \PY{n}{y}\PY{o}{=}\PY{n}{t2\PYZus{}s}\PY{p}{,} \PY{c+c1}{\PYZsh{}TODO}
                                            \PY{n}{z}\PY{o}{=}\PY{n}{z}\PY{p}{,} \PY{c+c1}{\PYZsh{}TODO}
                                            \PY{n}{colorscale}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Viridis}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{reversescale}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
             
             
             \PY{c+c1}{\PYZsh{} YOUR CODE HERE}
             \PY{c+c1}{\PYZsh{}raise NotImplementedError()}
         
             \PY{n}{plotly}\PY{o}{.}\PY{n}{offline}\PY{o}{.}\PY{n}{iplot}\PY{p}{(}\PY{n}{go}\PY{o}{.}\PY{n}{Figure}\PY{p}{(}\PY{n}{data}\PY{o}{=}\PY{p}{[}\PY{n}{lr\PYZus{}loss\PYZus{}contours}\PY{p}{,} \PY{n}{thata\PYZus{}points}\PY{p}{]}\PY{p}{,} \PY{n}{layout}\PY{o}{=}\PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{title}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{title}\PY{p}{\PYZcb{}}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}36}]:} \PY{c+c1}{\PYZsh{} Run this}
         \PY{n}{contour\PYZus{}plot}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Gradient Descent with Static Learning Rate}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{ts}\PY{p}{,} \PY{n}{l2\PYZus{}loss}\PY{p}{,} \PY{n}{sin\PYZus{}model}\PY{p}{,} \PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{)}
\end{Verbatim}


    
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}37}]:} \PY{c+c1}{\PYZsh{}\PYZsh{} Run me}
         \PY{n}{contour\PYZus{}plot}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Gradient Descent with Decay Learning Rate}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{ts\PYZus{}decay}\PY{p}{,} \PY{n}{l2\PYZus{}loss}\PY{p}{,} \PY{n}{sin\PYZus{}model}\PY{p}{,} \PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{)}
\end{Verbatim}


    
    
    In the following cells, write down the answer to the following
questions: - How do you interpret the two contour plots? - Compare
contour plot and 3D plot, what are the pros and cons of each?

    Again, I see that in the the static learning rate, as time progresses,
the theta vector oscillates about the optimal theta, while for the
decaying learning rate, the theta vector steadily approaches the optimal
theta and thus does not "overshoot" the optimal theta. I think the
contour plot is much easier to visualize, whereas the 3D plot is much
harder to wrap my head around. However, the contour plot lacks a certain
depth in detail, since we cannot see the z-values between contours,
whereas in the 3D plot, we are able to.

    \subsection{How to Improve?}\label{how-to-improve}

\subsubsection{Question 5c (optional)}\label{question-5c-optional}

Try adding the two additional model parameters for phase and amplitude
that we ignored (see 3a). What are the optimal phase and amplitude
values for your four parameter model? Do you get a better loss?

    YOUR ANSWER HERE

    \subsubsection{Question 5d (optional)}\label{question-5d-optional}

It looks like our basic two parameter model, a combination of a linear
function and sinusoidal function, was able to almost perfectly fit our
data. It turns out that many real world scenarios come from relatively
simple models.

At the same time, the real world can be incredibly complex and a simple
model wouldn't work so well. Consider the example below; it is neither
linear, nor sinusoidal, nor quadratic.

Optional: Suggest how we could iteratively create a model to fit this
data and how we might improve our results.

Extra optional: Try and build a model that fits this data.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}38}]:} \PY{n}{x} \PY{o}{=} \PY{p}{[}\PY{p}{]}
         \PY{n}{y} \PY{o}{=} \PY{p}{[}\PY{p}{]}
         \PY{k}{for} \PY{n}{t} \PY{o+ow}{in} \PY{n}{np}\PY{o}{.}\PY{n}{linspace}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mi}{10}\PY{o}{*}\PY{n}{np}\PY{o}{.}\PY{n}{pi}\PY{p}{,} \PY{l+m+mi}{200}\PY{p}{)}\PY{p}{:}
             \PY{n}{r} \PY{o}{=} \PY{p}{(}\PY{p}{(}\PY{n}{t}\PY{p}{)}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2}\PY{p}{)}
             \PY{n}{x}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{r}\PY{o}{*}\PY{n}{np}\PY{o}{.}\PY{n}{cos}\PY{p}{(}\PY{n}{t}\PY{p}{)}\PY{p}{)}
             \PY{n}{y}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{r}\PY{o}{*}\PY{n}{np}\PY{o}{.}\PY{n}{sin}\PY{p}{(}\PY{n}{t}\PY{p}{)}\PY{p}{)}
         
         \PY{n}{plt}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{x}\PY{p}{,}\PY{n}{y}\PY{p}{)}  
         \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_78_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    YOUR ANSWER HERE

    \begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsection{6: Short Analytic Problems}\label{short-analytic-problems}

Let's work through some problems to solidify the foundations of gradient
descent. If these questions are hard, consider reviewing lecture and
supplementary materials.

\subsubsection{Question 6}\label{question-6}

Complete the problems below. \textbf{Show your work and solution in
LaTeX}. Here are some useful examples of LaTex syntax:

Summation: \(\sum_{i=1}^n a_i\)

Exponent: \(a^2\)

Fraction: \(\frac{a}{b}\)

Multiplication: \(a \cdot b\)

Derivative: \(\frac{\partial}{\partial a}\)

Symbols: \(\alpha, \beta, \theta\)

    \subsubsection{Convexity}\label{convexity}

\paragraph{Question 6a}\label{question-6a}

In \href{http://www.ds100.org/fa18/syllabus\#lecture-week-5}{lecture 8},
we introduced the idea of a convex function. Let \(h(x) = f(x) + g(x)\)
where \(f,g\) are convex functions. Prove that \(h\) is convex.

    By definition, if \(f\) is convex, then:
\[tf(a) + (1-t)f(b) \ge f(ta + (1-t)b)\]
\[\forall a, \forall b, t \in [0,1]\] The same can be said for \(g\):
\[tg(a) + (1-t)g(b) \ge g(ta + (1-t)b)\]
\[\forall a, \forall b, t \in [0,1]\]

If this is the case, we can combine the add the two equations together:
\[tg(a) + tf(a) + (1-t)g(b) + (1-t)f(b) \ge g(ta + (1-t)b) + h(ta + (1-t)b)\]

If we know that \(h(x) = f(x) + g(x)\), then we can simplify the
expression:
\[t(f(a) + g(a)) + (1-t)(f(b) + g(b)) \ge h(ta + (1-t)b) + g(ta + (1-t)b)\]
\[th(a) + (1-t)h(b) \ge h(ta + (1-t)b)\]

Thus, \(h\) also fulfills the definition of a convex function, and hence
is also convex.

    \subsubsection{Mutlivariable/vector calculus mechanical
problems}\label{mutlivariablevector-calculus-mechanical-problems}

\paragraph{Question 6b}\label{question-6b}

Show that the sum of the squared error
\[L(\textbf{w}) = ||\textbf{Xw}-\textbf{y}||_2^2\] can be expanded into
\[L(\textbf{w}) = \textbf{w}^T \textbf{X}^T \textbf{X w} - 2 \textbf{y}^T \textbf{X w} + \textbf{y}^T \textbf{y}\]
using vector/matrix notation.

    \[L(\textbf{w}) = ||\textbf{Xw}-\textbf{y}||_2^2\]
\[L(\textbf{w}) = \sum^n_{i=1}((Xw)_i-y_i)^2\]
\[L(\textbf{w}) = \sum^n_{i=1}((Xw)_i^2-2(Xw)_iy_i+y_i^2)\]
\[L(\textbf{w}) = \sum^n_{i=1}(Xw)_i^2-2\sum^n_{i=1}(Xw)_iy_i+\sum^n_{i=1}y_i^2\]
In squared error, \((\textbf{Xw})\) and \(\textbf{y}\) are \([n,1]\).
Therefore, to square every value in a vector \([n,1]\), you multiply the
transpose of the vector by itself \(([1,n] \cdot [n,1] = [1, 1]\), or a
scalar\()\): \[\sum^n_{i=1}(Xw)_i^2 = (Xw)^TXw= w^TX^TXw\]
\[\sum^n_{i=1}y_i^2 = y^Ty\] \[\sum^n_{i=1}(Xw)_iy_i = y^TXw\]

Therefore, if we plug everything in:
\[L(\textbf{w}) = \textbf{w}^T \textbf{X}^T \textbf{X w} - 2 \textbf{y}^T \textbf{X w} + \textbf{y}^T \textbf{y}\]

    \paragraph{Question 6c}\label{question-6c}

Solve for the optimal \(\textbf{w}\), assuming \(\textbf{X}\) is full
rank. Use the Matrix Derivative rules from
\href{http://www.ds100.org/fa18/syllabus\#lecture-week-6}{lecture 11}.

    \[L(\textbf{w}) = \textbf{w}^T \textbf{X}^T \textbf{X w} - 2 \textbf{y}^T \textbf{X w} + \textbf{y}^T \textbf{y}\]
To minimize loss, we have to take the gradient of \(L\) and find where
it is \(0\).
\[\nabla_w L(w)=0=\nabla_w(\textbf{w}^T \textbf{X}^T \textbf{X w}) - 2\nabla_w(\textbf{y}^T \textbf{X w}) + \nabla_w(\textbf{y}^T \textbf{y})\]
\[0= \textbf{X}^T\textbf{Xw} + (\textbf{X}^T\textbf{X})^T\textbf{w} - 2(\textbf{y}^T)^T\]
\[0= 2\textbf{X}^T\textbf{Xw} - 2\textbf{y}\]
\[\textbf{w} = (\textbf{X}^T\textbf{X})^{-1}\textbf{y}\]

    \paragraph{Question 6d}\label{question-6d}

Repeat the steps above for ridge regression as described in
\href{http://www.ds100.org/fa18/syllabus\#lecture-week-6}{lecture 12}.
Recall that ridge regression uses the following l2 regularized sum of
squared error.

\[L(\textbf{w}) = ||\textbf{X}\textbf{w} - \textbf{y}||_2^2 + \lambda ||\textbf{w}||_2^2\]

    \[L(\textbf{w}) = ||\textbf{X}\textbf{w} - \textbf{y}||_2^2 + \lambda ||\textbf{w}||_2^2\]
As seen in 6b:
\[L(\textbf{w}) = \textbf{w}^T \textbf{X}^T \textbf{X w} - 2 \textbf{y}^T \textbf{X w} + \textbf{y}^T \textbf{y}+\lambda \sum^n_{i=1}(w_i)^2\]
Since \(\textbf{w}\) is also an \([n,1]\) vector,
\(\sum^n_{i=1}(w_i)^2 = \textbf{w}^T\textbf{w}\), since
\(([1,n] \cdot [n,1] = [1, 1]\), or a scalar\()\).
\[L(\textbf{w}) = \textbf{w}^T \textbf{X}^T \textbf{X w} - 2 \textbf{y}^T \textbf{X w} + \textbf{y}^T \textbf{y}+\lambda \textbf{w}^T\textbf{w}\]

To minimize loss, we have to take the gradient of \(L\) and find where
it is \(0\).
\[\nabla_w L(w)=0=\nabla_w(\textbf{w}^T \textbf{X}^T \textbf{X w}) - 2\nabla_w(\textbf{y}^T \textbf{X w}) + \nabla_w(\textbf{y}^T \textbf{y})+\lambda \Delta_w(\textbf{w}^T\textbf{I}\textbf{w})\]
\[0= 2\textbf{X}^T\textbf{Xw} - 2\textbf{y}+\lambda(\textbf{Iw} + \textbf{Iw})\]
\[ \textbf{y}= \textbf{X}^T\textbf{Xw}+\lambda\textbf{Iw}\]
\[\textbf{w} = (\textbf{X}^T\textbf{X}+\lambda\textbf{I})^{-1}\textbf{y}\]

    \paragraph{Question 6e}\label{question-6e}

Compare the analytic solutions of least squares and ridge regression.
Why does ridge regression guarantee that we can find a unique solution?
What are some of the tradeoffs (pros/cons) of using ridge regression?

    The analytic solutions of least squares and ridge regression are almost
identical, except the inverse matrix for the ridge regression has an
additional \(+\lambda\textbf{I}\) term. In the least squares solution,
\(\textbf{X}\) must be full rank so that an optimal \(\textbf{w}\)
exists. However, in ridge regression, this is not the case, since the
\(+\lambda\textbf{I}\) will almost always ensure that
\(\textbf{X}^T\textbf{X}+\lambda\textbf{I}\) will be invertable, because
if \(\textbf{X}^T\textbf{X}\) is not full rank (i.e. there are repeated
rows), the addition of \(+\lambda\textbf{I}\) ensure that all rows will
be different. This is a pro of using ridge regression, but this also
introduces a bias to the model, since we are prefering lower complexity
models prior to actually seeing the data. If you do not want bias, using
ridge regression is definately a con and one should opt for least
squares.

    \subsubsection{Expectation and Variance}\label{expectation-and-variance}

\paragraph{Question 6f}\label{question-6f}

In \href{http://www.ds100.org/fa18/syllabus\#lecture-week-6}{lecture
10}, we completed half of the proof for the linearity of expectation.
Your task in this question is to complete the second half.

For reference, in lecture we showed that:

\[\mathbb{E}[aX + bY + c] = a\mathbb{E}[X] + \sum_{x \in \mathbb{X}}\sum_{y \in \mathbb{Y}}P(x, y)by + c\]

To complete this proof, prove that:

\[b\mathbb{E}[Y] = \sum_{x \in \mathbb{X}}\sum_{y \in \mathbb{Y}}P(x, y)by\]

Note: You cannot simply start with the given equation and use linearity
of expectation. Start with the summation on the right side and
manipulate it to get the left side.

Hint: What can we do with the order of the summations?

    \[\sum_{x \in \mathbb{X}}\sum_{y \in \mathbb{Y}}P(x, y)by\]

\[\sum_{(x,y) \in (\mathbb{X}, \mathbb{Y})}P(x, y)by\]

\[\sum_{y \in \mathbb{Y}}\sum_{x \in \mathbb{X}}P(x, y)by\]

\[b\sum_{y \in \mathbb{Y}}y\sum_{x \in \mathbb{X}}P(x, y)\]

The second summation \(\sum_{x \in \mathbb{X}}P(x, y)\) is the sum of
all the probabilities for all \(x\), for a fixed \(y\). In other words,
\(\sum_{x \in \mathbb{X}}P(x, y)\) is the probability of \(y\), or
\(P(y)\).

\[b\sum_{y \in \mathbb{Y}}y P(y)\]

\[b\mathbb{E}[Y]\]

Therefore,
\[b\mathbb{E}[Y] = \sum_{x \in \mathbb{X}}\sum_{y \in \mathbb{Y}}P(x, y)by\]

    \paragraph{Question 6g}\label{question-6g}

Prove that if two random variables \(X\) and \(Y\) are independent, then
\(Var(X - Y) = Var(X) + Var(Y)\).

    By definition: \[Var(X-Y) = E[(X-Y)^2] - (E[X-Y])^2\]
\[Var(X-Y) = E[X^2 - 2XY + Y^2] - (E[X]-E[Y])^2\]
\[Var(X-Y) = E[X^2] - 2E[XY] + E[Y^2] - E[X]^2 + 2E[X]E[Y] - E[Y]^2\]
\[Var(X-Y) = (E[X^2] - E[X]^2) + (E[Y^2] - E[Y]^2) - 2E[X]E[Y]+ 2E[X]E[Y]\]
\[Var(X - Y) = Var(X) + Var(Y)\]

    \subsection{7: Quick Regex Problems}\label{quick-regex-problems}

Here are some quick problems to review your knowledge of regular
expressions.

    \paragraph{Question 7a}\label{question-7a}

Write a regular expression to match the following strings without using
the \texttt{\textbar{}} operator.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Match:} \texttt{abcdefg}
\item
  \textbf{Match:} \texttt{abcde}
\item
  \textbf{Match:} \texttt{abc}
\item
  \textbf{Skip:} \texttt{c\ abc}
\end{enumerate}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}39}]:} \PY{n}{regxa} \PY{o}{=} \PY{l+s+sa}{r}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZca{}abc.*}\PY{l+s+s2}{\PYZdq{}} \PY{c+c1}{\PYZsh{} fill in your pattern}
         \PY{c+c1}{\PYZsh{} YOUR CODE HERE}
         \PY{c+c1}{\PYZsh{}raise NotImplementedError()}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}40}]:} \PY{k}{assert} \PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{|}\PY{l+s+s2}{\PYZdq{}} \PY{o+ow}{not} \PY{o+ow}{in} \PY{n}{regxa}\PY{p}{)}
         \PY{k}{assert} \PY{p}{(}\PY{n}{re}\PY{o}{.}\PY{n}{search}\PY{p}{(}\PY{n}{regxa}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{abc}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{o}{.}\PY{n}{group}\PY{p}{(}\PY{p}{)} \PY{o}{==} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{abc}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)} 
         \PY{k}{assert} \PY{p}{(}\PY{n}{re}\PY{o}{.}\PY{n}{search}\PY{p}{(}\PY{n}{regxa}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{abcde}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{o}{.}\PY{n}{group}\PY{p}{(}\PY{p}{)} \PY{o}{==} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{abcde}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)} 
         \PY{k}{assert} \PY{p}{(}\PY{n}{re}\PY{o}{.}\PY{n}{search}\PY{p}{(}\PY{n}{regxa}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{abcdefg}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{o}{.}\PY{n}{group}\PY{p}{(}\PY{p}{)} \PY{o}{==} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{abcdefg}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{k}{assert} \PY{p}{(}\PY{n}{re}\PY{o}{.}\PY{n}{search}\PY{p}{(}\PY{n}{regxa}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{c abc}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)} \PY{o+ow}{is} \PY{k+kc}{None}\PY{p}{)}
\end{Verbatim}


    \paragraph{Question 7b}\label{question-7b}

Write a regular expression to match the following strings without using
the \texttt{\textbar{}} operator.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Match:} \texttt{can}
\item
  \textbf{Match:} \texttt{man}
\item
  \textbf{Match:} \texttt{fan}
\item
  \textbf{Skip:} \texttt{dan}
\item
  \textbf{Skip:} \texttt{ran}
\item
  \textbf{Skip:} \texttt{pan}
\end{enumerate}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}41}]:} \PY{n}{regxb} \PY{o}{=} \PY{l+s+sa}{r}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{[cmf]+an}\PY{l+s+s2}{\PYZdq{}} \PY{c+c1}{\PYZsh{} fill in your pattern}
         \PY{c+c1}{\PYZsh{} YOUR CODE HERE}
         \PY{c+c1}{\PYZsh{}raise NotImplementedError()}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}42}]:} \PY{k}{assert} \PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{|}\PY{l+s+s2}{\PYZdq{}} \PY{o+ow}{not} \PY{o+ow}{in} \PY{n}{regxb}\PY{p}{)}
         \PY{k}{assert} \PY{p}{(}\PY{n}{re}\PY{o}{.}\PY{n}{match}\PY{p}{(}\PY{n}{regxb}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{can}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{o}{.}\PY{n}{group}\PY{p}{(}\PY{p}{)} \PY{o}{==} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{can}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)} 
         \PY{k}{assert} \PY{p}{(}\PY{n}{re}\PY{o}{.}\PY{n}{match}\PY{p}{(}\PY{n}{regxb}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{fan}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{o}{.}\PY{n}{group}\PY{p}{(}\PY{p}{)} \PY{o}{==} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{fan}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)} 
         \PY{k}{assert} \PY{p}{(}\PY{n}{re}\PY{o}{.}\PY{n}{match}\PY{p}{(}\PY{n}{regxb}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{man}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{o}{.}\PY{n}{group}\PY{p}{(}\PY{p}{)} \PY{o}{==} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{man}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)} 
         \PY{k}{assert} \PY{p}{(}\PY{n}{re}\PY{o}{.}\PY{n}{match}\PY{p}{(}\PY{n}{regxb}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{dan}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)} \PY{o+ow}{is} \PY{k+kc}{None}\PY{p}{)} 
         \PY{k}{assert} \PY{p}{(}\PY{n}{re}\PY{o}{.}\PY{n}{match}\PY{p}{(}\PY{n}{regxb}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ran}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)} \PY{o+ow}{is} \PY{k+kc}{None}\PY{p}{)} 
         \PY{k}{assert} \PY{p}{(}\PY{n}{re}\PY{o}{.}\PY{n}{match}\PY{p}{(}\PY{n}{regxb}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{pan}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)} \PY{o+ow}{is} \PY{k+kc}{None}\PY{p}{)}
\end{Verbatim}


    \paragraph{Question 7c:}\label{question-7c}

Write a regular expression to extract and print the quantity and type of
objects in a string. You may assume that a space separates quantity and
type, ie. \texttt{"\{quantity\}\ \{type\}"}. See the example string
below for more detail.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Hint:} use \texttt{re.findall}
\item
  \textbf{Hint:} use \texttt{\textbackslash{}d} for digits and one of
  either \texttt{*} or \texttt{+}.
\end{enumerate}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}43}]:} \PY{n}{text\PYZus{}qc} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{I}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{ve got 10 eggs that I stole from 20 gooses belonging to 30 giants.}\PY{l+s+s2}{\PYZdq{}}
         
         \PY{n}{res\PYZus{}qc} \PY{o}{=} \PY{n}{re}\PY{o}{.}\PY{n}{findall}\PY{p}{(}\PY{l+s+sa}{r}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZbs{}}\PY{l+s+s2}{d+ [a\PYZhy{}z]*}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{text\PYZus{}qc}\PY{p}{)}
         \PY{c+c1}{\PYZsh{} YOUR CODE HERE}
         \PY{c+c1}{\PYZsh{}raise NotImplementedError()}
         
         \PY{n}{res\PYZus{}qc}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}43}]:} ['10 eggs', '20 gooses', '30 giants']
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}44}]:} \PY{k}{assert} \PY{n}{res\PYZus{}qc} \PY{o}{==} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{10 eggs}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{20 gooses}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{30 giants}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
\end{Verbatim}


    \paragraph{Question 7d:}\label{question-7d}

Write a regular expression to replace at most 2 occurrences of space,
comma, or dot with a colon.

\textbf{Hint:} use
\texttt{re.sub(regex,\ "newtext",\ string,\ number\_of\_occurences)}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}45}]:} \PY{n}{text\PYZus{}qd} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Python Exercises, PHP exercises.}\PY{l+s+s1}{\PYZsq{}}
         \PY{n}{res\PYZus{}qd} \PY{o}{=} \PY{n}{re}\PY{o}{.}\PY{n}{sub}\PY{p}{(}\PY{l+s+sa}{r}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{[ ,.]}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{text\PYZus{}qd}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)} \PY{c+c1}{\PYZsh{} Hint: use re.sub()}
         \PY{c+c1}{\PYZsh{} YOUR CODE HERE}
         \PY{c+c1}{\PYZsh{}raise NotImplementedError()}
         
         \PY{n}{res\PYZus{}qd}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}45}]:} 'Python:Exercises: PHP exercises.'
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}46}]:} \PY{k}{assert} \PY{n}{res\PYZus{}qd} \PY{o}{==} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Python:Exercises: PHP exercises.}\PY{l+s+s1}{\PYZsq{}}
\end{Verbatim}


    \paragraph{Question 7e (optional):}\label{question-7e-optional}

Write a regular expression to replace all words that are not
\texttt{"mushroom"} with \texttt{"badger"}.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}47}]:} \PY{n}{text\PYZus{}qe} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{this is a word mushroom mushroom}\PY{l+s+s1}{\PYZsq{}}
         \PY{n}{res\PYZus{}qe} \PY{o}{=} \PY{n}{re}\PY{o}{.}\PY{n}{sub}\PY{p}{(}\PY{l+s+sa}{r}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZbs{}}\PY{l+s+s2}{b(?!mushroom| )}\PY{l+s+s2}{\PYZbs{}}\PY{l+s+s2}{b}\PY{l+s+s2}{\PYZbs{}}\PY{l+s+s2}{S+}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{badger}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{text\PYZus{}qe}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{)} \PY{c+c1}{\PYZsh{} Hint: https://www.regextester.com/94017}
         \PY{c+c1}{\PYZsh{} YOUR CODE HERE}
         \PY{c+c1}{\PYZsh{}raise NotImplementedError()}
         \PY{n}{res\PYZus{}qe}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}47}]:} 'badger badger badger badger mushroom mushroom'
\end{Verbatim}
            
    \subsection{Submission - IMPORTANT, PLEASE
READ}\label{submission---important-please-read}

For this assignment and future assignments (homework and projects) you
will also submit your free response and plotting questions to
gradescope. To do this, you can download as PDF
(\texttt{File-\textgreater{}Download\ As-\textgreater{}PDF\ via\ Latex\ (.pdf)}).
You are responsible for submitting and tagging your answers in
gradescope. For each free response and plotting question, please
include:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Relevant code used to generate the plot or inform your insights
\item
  The written free response or plot
\end{enumerate}

We are doing this to make it easier on our graders and for you, in the
case you need to submit a regrade request. Gradescope (as of now) is
still better for manual grading.

    \subsection{Submission}\label{submission}

You're done!

Before submitting this assignment, ensure to:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Restart the Kernel (in the menubar, select
  Kernel-\textgreater{}Restart \& Run All)
\item
  Validate the notebook by clicking the "Validate" button
\end{enumerate}

Finally, make sure to \textbf{submit} the assignment via the Assignments
tab in Datahub


    % Add a bibliography block to the postdoc
    
    
    
    \end{document}
